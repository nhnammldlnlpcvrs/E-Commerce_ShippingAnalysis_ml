{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb99970",
   "metadata": {},
   "source": [
    "# Modeling for E-Commerce Shipping Data\n",
    "\n",
    "This notebook covers:\n",
    "1. Data loading and preprocessing pipeline.\n",
    "\n",
    "2. Data scaling & encoding checks.\n",
    "3. Pre-Modeling: metrics, thresholds, candidate models, hypotheses.\n",
    "4. Post-Modeling: training, evaluation, visualization, and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe57ba",
   "metadata": {},
   "source": [
    "## Load dataset and inspect structure\n",
    "We load the dataset, separate features and target, and identify numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f75bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/train_split.csv\")\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "display(df.head())\n",
    "\n",
    "# Separate X and y\n",
    "X = df.drop(\"Reached.on.Time_Y.N\", axis=1)\n",
    "y = df[\"Reached.on.Time_Y.N\"]\n",
    "\n",
    "numerical_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"Numerical features:\", numerical_features)\n",
    "print(\"Categorical features:\", categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d249f",
   "metadata": {},
   "source": [
    "## Build preprocessing pipeline\n",
    "- Numerical: StandardScaler\n",
    "\n",
    "- Categorical: OneHotEncoder\n",
    "Both combined into a ColumnTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e18c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "cat_pipeline = Pipeline([(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, numerical_features),\n",
    "    (\"cat\", cat_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "print(\"Data preprocessing pipeline created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c562e4",
   "metadata": {},
   "source": [
    "## Scaling check (before vs after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb277cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_before = X[numerical_features]\n",
    "X_num_after = num_pipeline.fit_transform(X_num_before)\n",
    "X_num_after_df = pd.DataFrame(X_num_after, columns=numerical_features)\n",
    "\n",
    "for col in numerical_features:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    sns.histplot(X_num_before[col], ax=axes[0], kde=True)\n",
    "    axes[0].set_title(f\"{col} - Before Scaling\")\n",
    "    sns.histplot(X_num_after_df[col], ax=axes[1], kde=True)\n",
    "    axes[1].set_title(f\"{col} - After Scaling\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad584b",
   "metadata": {},
   "source": [
    "## Encoding check: number of columns & sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efadfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohe.fit(X[categorical_features])\n",
    "num_new_columns = len(ohe.get_feature_names_out())\n",
    "print(\"Number of new columns from categorical encoding:\", num_new_columns)\n",
    "\n",
    "X_cat_transformed = ohe.transform(X[categorical_features])\n",
    "if X_cat_transformed.shape[1] > 0:\n",
    "    sparsity = 1 - (X_cat_transformed.nnz / (X_cat_transformed.shape[0] * X_cat_transformed.shape[1]))\n",
    "    print(f\"Sparsity after encoding: {sparsity:.4f}\")\n",
    "else:\n",
    "    print(\"No categorical features found, skipping sparsity calc.\")\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "print(\"Shape before preprocessing:\", X.shape)\n",
    "print(\"Shape after preprocessing:\", X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa573dcc",
   "metadata": {},
   "source": [
    "## Modeling Phase\n",
    "This section includes both Pre-Modeling and Post-Modeling:\n",
    "- Metrics & thresholds definition\n",
    "\n",
    "- Model selection & hypothesis\n",
    "- Model training & evaluation\n",
    "- Visualization & summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17979036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score,\n",
    "    \"Precision\": precision_score,\n",
    "    \"Recall\": recall_score,\n",
    "    \"F1 Score\": f1_score,\n",
    "    \"ROC-AUC\": roc_auc_score\n",
    "}\n",
    "print(\"Metrics selected:\", list(metrics.keys()))\n",
    "\n",
    "# Define thresholds\n",
    "thresholds = {\n",
    "    \"Accuracy\": 0.80,\n",
    "    \"Precision\": 0.75,\n",
    "    \"Recall\": 0.70,\n",
    "    \"F1 Score\": 0.72,\n",
    "    \"ROC-AUC\": 0.80\n",
    "}\n",
    "print(\"\\nThresholds:\")\n",
    "for m, t in thresholds.items():\n",
    "    print(f\"{m}: â‰¥ {t}\")\n",
    "\n",
    "# Candidate models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "}\n",
    "\n",
    "# Notes before training\n",
    "notes = {\n",
    "    \"Logistic Regression\": \"Simple, interpretable, may struggle with non-linearity.\",\n",
    "    \"Random Forest\": \"Captures non-linear patterns, robust to noise, less interpretable.\"\n",
    "}\n",
    "print(\"\\nðŸ“Œ Pre-training notes:\")\n",
    "for name, note in notes.items():\n",
    "    print(f\"{name}: {note}\")\n",
    "\n",
    "print(\"\\nHypothesis: Random Forest may outperform Logistic Regression.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b178a",
   "metadata": {},
   "source": [
    "## Model Training, Evaluation & Visualization\n",
    "We'll train both models, evaluate them on all metrics, plot ROC and Precision-Recall curves, and summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bad224",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "plt.figure()\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Store metrics\n",
    "    row = {\"Model\": name}\n",
    "    for m, func in metrics.items():\n",
    "        if m == \"ROC-AUC\" and y_proba is not None:\n",
    "            row[m] = func(y_test, y_proba)\n",
    "        elif m == \"ROC-AUC\":\n",
    "            row[m] = np.nan\n",
    "        else:\n",
    "            row[m] = func(y_test, y_pred)\n",
    "    results.append(row)\n",
    "\n",
    "    # ROC curve\n",
    "    if y_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc(fpr, tpr):.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "plt.figure()\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "        plt.plot(recall, precision, label=name)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFinal Evaluation Summary:\")\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-commerce_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
